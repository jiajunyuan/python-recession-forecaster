{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recession Forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The United States as of today (October, 2019) has experienced the longest expansion cycle, yet the there are constant talks in the news about an impending recession. Yes, there are signs, such as the recent yield curve inversion, the market corrections in 2018 & 2019, etc. Recessions happen every 10 years or so, are we overdue for one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part is to predict GDP growth, because NBER defines recessions as ['significant decline in economic activity'](https://www.nber.org/cycles.html), not 2 consequtive quarters of deline in real GDP. This means GDP can be growing, albeit at a slow pace, and the economy is still considered in recession.\n",
    "\n",
    "But due to limitations of the VAR model employed in part 1, it is necessary to employ an alternative method, to ______."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are techniques and algorithms to systematically select variables, this is outside the scope of this analysis. Instead, we will rely on domain knowledge. The following factors are considered when selecting input variables:\n",
    "* Having sufficient data, as recessions are rare occurances.\n",
    "* Having forward predictive power, which means inflection changes should occur before occur simultaneously as inflection changes in GDP.\n",
    "* Represent various aspects and mechanisms of the economy, such as a risk free benchmark, or inflation.\n",
    "* Avoid composite models or indices, as the goal is to construct our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GDP decision variable:\n",
    "* [**Real Gross Domestic Product**](https://fred.stlouisfed.org/series/GDPC1)- Inflation adjusted value of goods and services produced in a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For equities, the following variables are considered:\n",
    "* [S&P 500](https://finance.yahoo.com/quote/%5EGSPC?p=^GSPC)- Weighted stock index of 500 companies listed on US exchanges with the largest market cap.\n",
    "* [S&P 500 Cyclically Adjusted PE Ratio](https://www.quandl.com/data/MULTPL/SHILLER_PE_RATIO_MONTH-Shiller-PE-Ratio-by-Month) [(info)](https://www.investopedia.com/terms/c/cape-ratio.asp)- Normalizes PE ratio fluctuations over 10 year inflation-adjusted earnings. This indicator is used to gauge whether the equities market is over or under-valued.\n",
    "* [**Nonfinancial Corporate Debt as Percentage of Equity**](https://fred.stlouisfed.org/series/NCBCMDPMVCE)- Contains companies not in the S&P 500. Although this indicator excludes [% of GDP](), valuations of financial and noncorporate companies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bonds, the following variables are considered: \n",
    "* [10 Year Constant Maturity Minus 3 Month Treasuries Yield Spread](https://fred.stlouisfed.org/series/T10Y3M)- Federal Reserve's main methodology. Financial institutions borrow at low rates short term, to lend at high rates long term. Long term treasury yield tends to be stable, but short term yields are more volatile, and subject to [Federal Reserve interventions]((https://fredblog.stlouisfed.org/2017/02/lets-do-the-twist/)). The data only goes back to 1982, which is not enough to train this model.\n",
    "* [**10 Year Constant Maturity Minus 3 Month Treasuries Secondary Market Yield Spread**](https://fred.stlouisfed.org/series/TB3MS)- Makes up for the lack of data in the primary market. Primary and secondary market spreads are very close today, due to increased efficiency from electronic trading. Historically, the spread has been higher.\n",
    "* [10 Year Treasuries Constant Maturity Rate](https://fred.stlouisfed.org/series/DGS10)- Long term treasuries usually reflect investor sentiment regarding long term economic growth. However, this metric alone does not provide evidence of short term liquidity.\n",
    "* 3 Months Outstanding Repo [(info)](http://law.emory.edu/ecgar/content/volume-5/issue-2/essays/repo-recession-financial-regulation.html)- Before the Great Recession, investment banks used short term repo to inject liquidity to stay afloat. Uptick in short term repo may indicate credit crunch. FRED only has records of contracts with itself as a participant, leaving out the majority of transactions. Data on market transactions were difficult to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inflation, the following indicators are considered:\n",
    "* [**Consumer Price Index for Urban Consumers**](https://fred.stlouisfed.org/series/CPIAUCSL) [(info)](https://www.bls.gov/opub/btn/volume-3/why-does-bls-provide-both-the-cpi-w-and-cpi-u.htm)- Price inflation covering 88% of Americans, calculated from expenditures. Uses a survey to calculate the basket of goods and services. Basket is updated every 2 years. Selected due to difficulty quantifying intangible traits such as quality.\n",
    "* [Chained Consumper Price Index for Urban Consumers](https://fred.stlouisfed.org/series/SUUR0000SA0) [(info)](https://www.brookings.edu/blog/up-front/2017/12/07/the-hutchins-center-explains-the-chained-cpi/)- Lacking long term data. Similar to CPI-U, but considers substitution purchases, and weights the changes every month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For employment, the following indicators are considered:\n",
    "* [Labor Participation Rate](https://fred.stlouisfed.org/series/CIVPART)- Percent of population over 16 actively seeking or engaged in employment. Too macro driven, such as women participation in the workforce, or retirement of Baby Boomers.\n",
    "* [**Total Nonfarm Payroll**](https://fred.stlouisfed.org/series/PAYEMS)- Size of the labor force, accounting for 80% of workers who contribute to GDP, excluding propritors, unpaid volunteers, or farm workers.\n",
    "* [Wage Growth](https://fred.stlouisfed.org/series/CES0500000003)- When labor demand outstrips supply, wages increase, and vise versa. Lack of long term data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the following indicators are not placed in any categories above, but are included because of their predictive power.\n",
    "* [**Real Private Gross Investment**](https://fred.stlouisfed.org/series/GPDIC1)- Investment represents expenditure on capital goods and residential properties. Provides an indicator for future productivity and GDP growth. Also a strong sign of economic recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following indicators were considered, but ultimately dismissed.\n",
    "* [Purchaing Managers' Index](https://www.instituteforsupplymanagement.org/ismreport/mfgrob.cfm?SSO=1)- Locked behind paywall. A survey of supply chain professionals in 19 primary industries.\n",
    "* [Composite Index of Leading Indicators](https://www.conference-board.org/data/bcicountry.cfm?cid=1) [(info)](https://www.investopedia.com/terms/c/cili.asp)- Locked behind paywall. A composite index of 10 indicators.\n",
    "* [Consumer Confidence Index](https://www.conference-board.org/data/consumerconfidence.cfm)- Locked behind paywall. A survey of consumer purchases and sentiments.\n",
    "* [VIX Volativity Index](https://fred.stlouisfed.org/series/VIXCLS)- Describes the volatility, but not the direction, of stocks. Also does not describe economic strength.\n",
    "* [Effective Federal Funds Rate](https://fred.stlouisfed.org/series/FEDFUNDS)- The most inorganic bond indicator, as the FED has direct control of this rate, which means it is unfiltered by market forces. Secondly, different FED chairs have different doctrines. The FED was fighting inflation in the 70s, deregulation in the 80s, inflation targeting in the 90s & 2000s, then quantitative easing in the 2010s.\n",
    "* [Personal Savings Rate](https://fred.stlouisfed.org/series/PSAVERT)- Too macro driven, such as women entering workforce, or retirement of Baby Boomers.\n",
    "* Incremental Capital Outputs Ratio- Hard to find on the web. Calculates how much additional capital investment is needed to create growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fredapi\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sb\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from copy import deepcopy\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import FRED Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import FRED data, we need an API key. API key can be requested [here](https://research.stlouisfed.org/docs/api/fred/).\n",
    "\n",
    "Data returned by fredapi module are in Series format. We need to convert them to DataFrame format.\n",
    "\n",
    "We will also rename the columns so the data become easier to manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fredapi.Fred(api_key='8b91217446b6307d20cb5e4fcfba70eb') # remove API when done\n",
    "\n",
    "# monthly data.\n",
    "tbill_3m = f.get_series('TB3MS').to_frame().reset_index().rename(columns={'index':'date', 0:'tbill_3m'})\n",
    "tbill_10y = f.get_series('GS10').to_frame().reset_index().rename(columns={'index':'date', 0:'tbill_10y'})\n",
    "cpiu = f.get_series('CPIAUCSL').to_frame().reset_index().rename(columns={'index':'date', 0:'cpiu'})\n",
    "payroll = f.get_series('PAYEMS').to_frame().reset_index().rename(columns={'index':'date', 0:'payroll'})\n",
    "# quarterly data\n",
    "dtoe = f.get_series('NCBCMDPMVCE').to_frame().reset_index().rename(columns={'index':'date', 0:'dtoe'})\n",
    "pinvest = f.get_series('GPDIC1').to_frame().reset_index().rename(columns={'index':'date', 0:'pinvest'})\n",
    "gdp = f.get_series('GDPC1').to_frame().reset_index().rename(columns={'index':'date', 0:'gdp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Quandl Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl_api_key = '8ufKe7Y2JMsYPU3CGN7m'\n",
    "sp500 = pd.DataFrame.from_dict(requests.get('https://www.quandl.com/api/v3/datasets/MULTPL/SP500_REAL_PRICE_MONTH.json?api_key={}'.format('8ufKe7Y2JMsYPU3CGN7m')).json()['dataset']['data'])\n",
    "sp500_cape = pd.DataFrame.from_dict(requests.get('https://www.quandl.com/api/v3/datasets/MULTPL/SHILLER_PE_RATIO_MONTH.json?api_key={}'.format('8ufKe7Y2JMsYPU3CGN7m')).json()['dataset']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrx/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py:346: FutureWarning: Passing in 'datetime64' dtype with no precision is deprecated\n",
      "and will raise in a future version. Please pass in\n",
      "'datetime64[ns]' instead.\n",
      "  data, copy=copy, name=name, dtype=dtype, **kwargs\n"
     ]
    }
   ],
   "source": [
    "# reverse order\n",
    "# index date\n",
    "# rename columns\n",
    "# sp500 = sp500.rename(columns={0:'date', 1:'sp500'})\n",
    "# sp500['date'] = pd.to_datetime(sp500['date'])\n",
    "# sp500 = sp500[::-1].set_index('date')\n",
    "# sp500_cape = sp500_cape[::-1].rename(columns={0:'date', 1:'sp500_cape'})\n",
    "sp500_cape = sp500_cape[::-1].rename(columns={0:'date', 1:'sp500'}).set_index('date')\n",
    "sp500_cape.index = sp500_cape.index.astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1787 entries, 1871-02-01 to 2019-11-01\n",
      "Data columns (total 1 columns):\n",
      "sp500    1787 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 27.9 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sp500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1871-02-01</td>\n",
       "      <td>10.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-03-01</td>\n",
       "      <td>11.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-04-01</td>\n",
       "      <td>12.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-05-01</td>\n",
       "      <td>12.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-06-01</td>\n",
       "      <td>12.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>28.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>28.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>28.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>29.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1787 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sp500\n",
       "date             \n",
       "1871-02-01  10.92\n",
       "1871-03-01  11.19\n",
       "1871-04-01  12.05\n",
       "1871-05-01  12.59\n",
       "1871-06-01  12.59\n",
       "...           ...\n",
       "2019-07-01  29.99\n",
       "2019-08-01  28.65\n",
       "2019-09-01  28.64\n",
       "2019-10-01  28.76\n",
       "2019-11-01  29.82\n",
       "\n",
       "[1787 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sp500_cape.info())\n",
    "sp500_cape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1788 entries, 1871-01-01 to 2019-11-01\n",
      "Data columns (total 1 columns):\n",
      "sp500    1788 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 27.9 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sp500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1871-01-01</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-02-01</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-03-01</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-04-01</td>\n",
       "      <td>4.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871-05-01</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2996.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2883.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2906.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2940.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>3066.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1788 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sp500\n",
       "date               \n",
       "1871-01-01     4.44\n",
       "1871-02-01     4.50\n",
       "1871-03-01     4.61\n",
       "1871-04-01     4.74\n",
       "1871-05-01     4.86\n",
       "...             ...\n",
       "2019-07-01  2996.11\n",
       "2019-08-01  2883.98\n",
       "2019-09-01  2906.27\n",
       "2019-10-01  2940.25\n",
       "2019-11-01  3066.91\n",
       "\n",
       "[1788 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sp500.info())\n",
    "sp500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Start Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that variables have different startig dates, which we need to standardize; doing so also removes any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tbill_3m.head(1))\n",
    "print(tbill_10y.head(1))\n",
    "print(cpiu.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency, we will set the starting date at 1953/4/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('1953, 4')\n",
    "tbill_3m = tbill_3m[tbill_3m.iloc[:,0] >= start_date]\n",
    "tbill_10y = tbill_10y[tbill_10y.iloc[:,0] >= start_date]\n",
    "cpiu = cpiu[cpiu.iloc[:,0] >= start_date]\n",
    "payroll = payroll[payroll.iloc[:,0] >= start_date]\n",
    "pinvest = pinvest[pinvest.iloc[:,0] >= start_date]\n",
    "dtoe = dtoe[dtoe.iloc[:,0] >= start_date]\n",
    "gdp = gdp[gdp.iloc[:,0] >= start_date]\n",
    "sp500 = sp500[sp500.iloc[:,0] >= start_date]\n",
    "sp500_cape = sp_500[sp_500.iloc[:,0] >= start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving Correct Treasury Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRED have a graph, not series of data on [10 year - 3 month spread](https://fred.stlouisfed.org/graph/?g=oGg). We have to creat our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbill = [tbill_10y, tbill_3m]\n",
    "tbill = reduce(lambda left,right: pd.merge(left,right,on='date'), tbill)\n",
    "tbill = tbill.set_index('date')\n",
    "tbill['tbill_10y'] = tbill.loc[:,'tbill_10y'] - tbill.loc[:,'tbill_3m']\n",
    "tbill = tbill.drop('tbill_3m', axis=1)\n",
    "tbill = tbill.rename(columns={'tbill_10y':'tbill'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Quarterly to Monthly Data and Null Value Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert quarterly to monthly data, we will call the \"resample\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtoe = dtoe.set_index('date').resample('MS').asfreq()\n",
    "pinvest = pinvest.set_index('date').resample('MS').asfreq()\n",
    "gdp = gdp.set_index('date').resample('MS').asfreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newly resampled months have null values, we will use [cublic spline interpolation](https://columbiaeconomics.com/2010/01/20/how-economists-convert-quarterly-data-into-monthly-cubic-spline-interpolation/), a technique commonly used by economists, to fill in the missing values. Cubic spline has better smoothing over quadratic interpolation.\n",
    "\n",
    "Interpolation converts data to Series. We need to convert back to DataFrame type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = gdp['gdp'].interpolate(method='spline', order=3).to_frame().reset_index()\n",
    "dtoe = dtoe['dtoe'].interpolate(method='spline', order=3).to_frame().reset_index()\n",
    "pinvest = pinvest['pinvest'].interpolate(method='spline', order=3).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all variables into the the same dataframe for easy manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = [tbill, cpiu, payroll, dtoe, pinvest, gdp]\n",
    "data = reduce(lambda left,right: pd.merge(left,right,on='date'), data)\n",
    "data = data.set_index('date')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify there are no remainng null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 793 months of data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, dpi=120, figsize=(10,6))\n",
    "for col, ax in enumerate(axes.flatten()):\n",
    "    ax.plot(data[data.columns[col]], color='red', linewidth=1)\n",
    "    ax.set_title(data.columns[col])\n",
    "    ax.tick_params(labelsize=8)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "ax = sb.heatmap(data.corr(), xticklabels=data.columns.values, yticklabels=data.columns.values, annot=True, annot_kws={'size':12})\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPIU, PAYROLL, PINVEST, and GDP are highly correlated. This could create problems of multicollinearity. Multicollinearity reduces the statistical significance of the variables, making it difficult to pinpoint which variable is responsible for changes in GDP.\n",
    "\n",
    "Going back to the criteria set at the beginning, CPIU and PAYROLL will be removed. While PINVEST have larger variance, which may obsecure the signal, PINVEST is kept because it has the earliest inflection point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['cpiu', 'payroll'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dealing with multiple variables, a vector autoregression (VAR) model is right for the job.\n",
    "\n",
    "VAR is a stochastic (random) process that captures linear relationship between all variables. VAR assumes all variables are dependent on each other, and that change in 1 variable will induce change in another. This trait is called Granger causality. We can verify if we should reject this hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDP is highly autocorrelated. While this feature can allow a generalization of GDP trend (2% year-over-year growth), we are interested in the short term fluctuations. Having high autocorrelation does not help to delect these fluctuations.\n",
    "\n",
    "** Expalin full vs. partial autocorrelation **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(10, 6))\n",
    "ax[0] = plot_acf(data.gdp, ax=ax[0])\n",
    "ax[1] = plot_pacf(data.gdp, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important for all data to be stationary, meaning the mean and variance do not change overtime. The primary feature to eliminate is trend, for example: GDP increases steadily over the years, the mean increases as time increases, and so does the variance. At the current trend, as time approaches infinity, so does the mean and variance. Many data models do not deal with changing mean and variance very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StationarityTests:\n",
    "    def __init__(self, significance=.05):\n",
    "        self.SignificanceLevel = significance\n",
    "        \n",
    "    def ADF_Stationarity_Test(self, timeseries):\n",
    "        adfTest = adfuller(timeseries, autolag='AIC')\n",
    "        self.pValue = adfTest[1]\n",
    "        \n",
    "        if (self.pValue < self.SignificanceLevel):\n",
    "            self.isStationary = True\n",
    "        else:\n",
    "            self.isStationary = False\n",
    "        \n",
    "        dfResults = pd.Series(adfTest[0:4], index=['ADF Test Statistic','P-Value','# Lags','# Observations'])\n",
    "        dfResults['Critical Value 5%'] = adfTest[4]['5%']\n",
    "\n",
    "        print('Augmented Dickey-Fuller Test Results:')\n",
    "        print(dfResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sTest = StationarityTests()\n",
    "\n",
    "for var in data:\n",
    "    sTest.ADF_Stationarity_Test(data[var])\n",
    "    print('Is {} series stationary? {}'.format(var, sTest.isStationary))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our data stationary, we will take the first difference of month(t):\n",
    "y_t - y_(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = data.diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data Into Trainng and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_periods = 12\n",
    "train, test = data[0:-test_periods], data[-test_periods:]\n",
    "train_trans = data_trans[0:-test_periods]\n",
    "train_trans.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for autocorrelation, much have been eliminated, but some degrees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(10, 6))\n",
    "ax[0] = plot_acf(train_trans.gdp, ax=ax[0])\n",
    "ax[1] = plot_pacf(train_trans.gdp, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in train:\n",
    "    sTest.ADF_Stationarity_Test(train_trans[var])\n",
    "    print(\"Is {} series stationary? {}\".format(var, sTest.isStationary))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, dpi=120, figsize=(10,6))\n",
    "for col, ax in enumerate(axes.flatten()):\n",
    "    ax.plot(train_trans[train_trans.columns[col]], color='red', linewidth=1)\n",
    "    ax.set_title(train_trans.columns[col])\n",
    "    ax.tick_params(labelsize=8)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "ax = sb.heatmap(train_trans.corr(), xticklabels=train_trans.columns.values, yticklabels=train_trans.columns.values, annot=True, annot_kws={'size':12})\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag Order Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR uses 4 metrics for lag order selection: \n",
    "\n",
    "* Aikaike Information Criterion (AIC)\n",
    "* Baysian Information Criteron (BIC) \n",
    "* Final Prediction Error (FPE)\n",
    "* Hannan-Quinn Information Criterion (HQIC)\n",
    "\n",
    "Not getting into the specific mechanics, these four metrics return the error value. We want the lag orders with the lowest error value. \n",
    "\n",
    "https://stats.stackexchange.com/questions/246886/selecting-lag-order-for-var-and-vecm\n",
    "\n",
    "AIC has the tendency to chooose large lag order, while BIC has the tendency to choose small lag order. For that reason, the average of AIC and BIC will be used. The average lag order is 14.5, so we will select 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model = VAR(train_trans)\n",
    "train_model.select_order(maxlags=36).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Causality & Cointegration Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger causality test assumes there is no relationship between the lag value of x(row) and present value of y(column), if resultant p value is greater than 0.05.\n",
    "\n",
    "The heat map below indicates *pinvest* has no causality relations, but the p-value is not too far from the threshold. While this raises red flag regarding our model, implying different variable and transformation selection methods should be employed, we can verify the merit of this interpretation through cointegration test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_order = 21\n",
    "\n",
    "def grangerstest(data, variables, test='ssr_chi2test'):\n",
    "    res = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "\n",
    "    for c in data.columns:\n",
    "        for r in res.index:\n",
    "            granger_res = grangercausalitytests(data[[r, c]], maxlag=lag_order, verbose=False)\n",
    "            p_values = [round(granger_res[i+1][0][test][1],4) for i in range(lag_order)]\n",
    "\n",
    "            res.loc[r, c] = min(p_values)\n",
    "    res.columns = [var + '_x' for var in variables]\n",
    "    res.index = [var + '_y' for var in variables]\n",
    "    return res\n",
    "\n",
    "gct = grangerstest(train_trans, variables=train_trans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "ax = sb.heatmap(gct, xticklabels=gct.columns.values, yticklabels=gct.index.values, annot=True, annot_kws={'size':12})\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cointegration is simply that by lagging one variable against another (order of integration or lag order), the distance between variables is stationary throughout time. If two variables are cointegrated, there must be granger causality (even if a weak one, or one way causality).\n",
    "\n",
    "Johansen test combines two series together, than tests whether this combined series has less order of integration than a single variable on its own. If so, then the variables are cointegrated.\n",
    "\n",
    "Here we pass in 3 variables:\n",
    "1. data,\n",
    "2. 1 for stochastic nature of the data,\n",
    "3. number of lagged periods.\n",
    "\n",
    "While there are many literature on whether the economy is [deterministic or stochastic](https://fraser.stlouisfed.org/files/docs/historical/frbsf/frbsf_let/frbsf_let_19930212.pdf), with proponents of a deterministic model arguing GDP hinders on the long term supply factors, such as capital, labor, and technology, and that growth rate should stay the same overtime; while proponents of a stochastic model would point to \"permanent\" changes to the growth rate, positive or negative, such as sudden sharp slope increase in supply factors since the industrial revolution. Under a deterministic model, proponents of stochastic model would say, people would still be living in the 17th century."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cointegration_test(data, det_order, lag_order, significance=0.05):\n",
    "    out = coint_johansen(data, det_order, lag_order)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-significance)]]\n",
    "\n",
    "    def adjust(val, length=6): return str(val).ljust(length)\n",
    "    print('Name   ::  Test Stat  >  C(95%)    => Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(data.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace, 2), 9), '> ', adjust(cvt, 8), ' => ', trace > cvt)\n",
    "        \n",
    "cointegration_test(train_trans, det_order=-1, lag_order=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing lag order of 15, *gdp* fails the cointegration test, so we will decrease the lag order until all four variables are cointegrated. We do not want the lag order to be too small, as small lag orders tend to be autocorrelated.\n",
    "\n",
    "Below is a modified function to find the highest lag order that satisfies the cointegration test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cointegration_test_lag_order_detection(data, det_order, lag_order, significance=0.05):\n",
    "    out = coint_johansen(data, det_order, lag_order)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-significance)]]\n",
    "    res = traces > cvts\n",
    "    return res\n",
    "\n",
    "for i in reversed(range(1, 15)):\n",
    "    print('lag order:', i, cointegration_test_lag_order_detection(train_trans, det_order=-1, lag_order=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_order = 8\n",
    "train_model_fitted = train_model.fit(lag_order)\n",
    "train_model_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Residual Correlation with Durbin Watson Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our transformation and lag order selection is done correctly, there should be little to no correlation between the residual errors. The existance of residual correlation implies unexplained pattern.\n",
    "\n",
    "Result values can range beween 0 to 4, with 0-2 indicating a positive correlation, and 2-4 indicating negative correlation. It is generally acceptable for values to be between 1.5 to 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in zip(train_trans.columns, durbin_watson(train_model_fitted.resid)):\n",
    "    print((col), ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_forecast_input = train_trans.values[-lag_order:]\n",
    "train_forecast_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_forecast = pd.DataFrame(train_model_fitted.forecast(train_forecast_input, test_periods), \n",
    "                        index=data_trans.index[-test_periods:], \n",
    "                        columns=data_trans.columns)\n",
    "\n",
    "train_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverting Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_forecast_reverse = train_forecast.cumsum() + train.iloc[-1, :]\n",
    "train_forecast_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=int(len(train_forecast_reverse.columns)/2), ncols=2, dpi=120, figsize=(10,6))\n",
    "for i, (col,ax) in enumerate(zip(train_forecast_reverse.columns, axes.flatten())):\n",
    "    train_forecast_reverse[col].plot(color='red', label='prediction', legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    test[col].plot(color='blue', label='actual', legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason this model was not able "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MAPE- \n",
    "* ME- \n",
    "* MAE- \n",
    "* MPE- \n",
    "* RMSE- \n",
    "* Minmax- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual) / np.abs(actual))  # MAPE\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
    "    mpe = np.mean((forecast - actual) / actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins / maxs)             # minmax\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_forecast_accuracy(forecast, actual):\n",
    "    for c in forecast:\n",
    "        print('Forecast Accuracy of {}'.format(c))\n",
    "        accuracy_prod = forecast_accuracy(forecast[c].values, actual[c])\n",
    "        for k, v in accuracy_prod.items():\n",
    "            print(k, ': ', round(v,4))\n",
    "        print('\\n')\n",
    "\n",
    "print_forecast_accuracy(forecast=train_forecast_reverse, actual=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VAR(data_trans)\n",
    "# model_fitted = train_model.fit(lag_order)\n",
    "\n",
    "# forecast = pd.DataFrame(train_model_fitted.forecast(data_trans.values[-lag_order:], test_periods), \n",
    "#                         index=data_trans.index[-test_periods:],\n",
    "#                         columns=data_trans.columns)\n",
    "# forecast_reverse = forecast.cumsum() + data.iloc[-1, :]\n",
    "\n",
    "# forecast_reverse.index = pd.date_range(pd.to_datetime('2019, 05, 1'), pd.to_datetime('2020, 04, 1'), freq='MS')\n",
    "# forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=int(len(forecast_reverse.columns)/2), ncols=2, dpi=120, figsize=(10,6))\n",
    "# for i, (col,ax) in enumerate(zip(forecast_reverse.columns, axes.flatten())):\n",
    "#     test[col].plot(color='blue', label='actual', legend=True, ax=ax)\n",
    "#     forecast_reverse[col].plot(color='red', label='prediction', legend=True, ax=ax).autoscale(axis='x',tight=True);\n",
    "#     ax.set_title(col + \": Forecast\")\n",
    "#     ax.xaxis.set_ticks_position('none')\n",
    "#     ax.yaxis.set_ticks_position('none')\n",
    "#     ax.spines[\"top\"].set_alpha(0)\n",
    "#     ax.tick_params(labelsize=6)\n",
    "\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_combined(data_input, lag_order, start_date, test_periods):\n",
    "    '''return start date'''\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    \n",
    "    '''dates for graphing'''\n",
    "    data_baseline = data_input.loc[(data_input.index >= start_date - relativedelta(months=2 * test_periods)) & \n",
    "                                    (data_input.index <= start_date + relativedelta(months=test_periods))]\n",
    "    \n",
    "    '''take first first difference'''\n",
    "    data_diff = data_input.diff().dropna()\n",
    "    data_diff = data_diff.loc[data_diff.index <= start_date]\n",
    "\n",
    "    '''VAR model'''\n",
    "    model = VAR(data_diff)\n",
    "    model_fitted = train_model.fit(lag_order)\n",
    "    forecast_diff = pd.DataFrame(\n",
    "        model_fitted.forecast(data_diff.values[-lag_order:], test_periods), \n",
    "        index=data_diff.index[-test_periods:], columns=data_diff.columns\n",
    "    )\n",
    "    \n",
    "    '''invert transformation'''\n",
    "    '''boolean index returns dataframe, but needs to be series'''\n",
    "    forecast_reverse = forecast_diff.cumsum() + data_input.iloc[data_input.index == start_date].squeeze()\n",
    "#     forecast_reverse.index = pd.date_range(pd.to_datetime('2019, 05, 1'), pd.to_datetime('2020, 04, 1'), freq='MS')\n",
    "    forecast_reverse.index = pd.date_range(start_date + relativedelta(months=1), start_date + \n",
    "                                           relativedelta(months=test_periods), freq='MS')\n",
    "#     print(forecast_reverse.tail(1))\n",
    "#     print(data_diff.tail(1))\n",
    "#     print(data_input.tail(1))\n",
    "\n",
    "    '''display graphs'''\n",
    "    fig, axes = plt.subplots(nrows=int(len(forecast_reverse.columns)/2), ncols=2, dpi=120, figsize=(7,4))\n",
    "    for i, (col,ax) in enumerate(zip(forecast_reverse.columns, axes.flatten())):\n",
    "        data_baseline[col].plot(color='blue', label='actual', legend=True, ax=ax)\n",
    "        forecast_reverse[col].plot(color='red', label='prediction', legend=True, ax=ax).autoscale(axis='x',tight=True);\n",
    "        ax.set_title(col + \": Forecast\")\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        ax.spines[\"top\"].set_alpha(0)\n",
    "        ax.tick_params(labelsize=6)\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_combined(data, lag_order=8, start_date='2018, 4', test_periods=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predictor does not pinpoint the cause of a recession, but makes a general comment about the state of the economy. The underlying assumption is that these inputs are efficient enough to reflect long term market conditions, but inefficient enough to price in the short term fluctuations in GDP.\n",
    "\n",
    "When trying to quantify debt as percentage of equity, it turned out that noncoporate entities \n",
    "\n",
    "The model only accepts data input from the determined lag order, meaning only 15 months of data was used.  The limited dataset does not include a recession, meaning the w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up recession decision variable. Difficulty joining series, so had to convert to list, append, then reconvert to series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible to create lambda variables?\n",
    "r_1953 = pd.Series(pd.date_range(pd.to_datetime('1953, 3, 1'), pd.to_datetime('1954, 5, 1'))).tolist()\n",
    "r_1957 = pd.Series(pd.date_range(pd.to_datetime('1957, 8, 1'), pd.to_datetime('1958, 4, 1'))).tolist()\n",
    "r_1960 = pd.Series(pd.date_range(pd.to_datetime('1960, 4, 1'), pd.to_datetime('1961, 2, 1'))).tolist()\n",
    "r_1962 = pd.Series(pd.date_range(pd.to_datetime('1969, 12, 1'), pd.to_datetime('1970, 11, 1'))).tolist()\n",
    "r_1973 = pd.Series(pd.date_range(pd.to_datetime('1973, 11, 1'), pd.to_datetime('1975, 3, 1'))).tolist()\n",
    "r_1980 = pd.Series(pd.date_range(pd.to_datetime('1980, 1, 1'), pd.to_datetime('1980, 7, 1'))).tolist()\n",
    "r_1981 = pd.Series(pd.date_range(pd.to_datetime('1981, 7, 1'), pd.to_datetime('1982, 11, 1'))).tolist()\n",
    "r_1990 = pd.Series(pd.date_range(pd.to_datetime('1990, 7, 1'), pd.to_datetime('1991, 3, 1'))).tolist()\n",
    "r_2001 = pd.Series(pd.date_range(pd.to_datetime('2001, 3, 1'), pd.to_datetime('2001, 11, 1'))).tolist()\n",
    "r_2007 = pd.Series(pd.date_range(pd.to_datetime('2007, 12, 1'), pd.to_datetime('2009, 6, 1'))).tolist()\n",
    "\n",
    "print('Original series length:', len(r_1953))\n",
    "\n",
    "recession_months = pd.Series(r_1953 + r_1957 + r_1960 + r_1962 + r_1973 + r_1980 + r_1981 + r_1990 + r_2001 + r_2007)\n",
    "\n",
    "# Create a DataFrame for recession\n",
    "start_date = pd.to_datetime('1953, 4')\n",
    "end_date = pd.to_datetime('today')\n",
    "recessions = pd.date_range(start_date, end_date, freq='MS')\n",
    "recessions = recessions.to_frame().reset_index()\n",
    "\n",
    "# Mark recession months as 1, non-recession months as 0\n",
    "recessions.iloc[:,1] = 0\n",
    "recessions.loc[recessions.iloc[:,0].isin(recession_months), 0] = 1\n",
    "recessions.rename(columns={'index':'date', 0:'recession'}, inplace=True)\n",
    "\n",
    "\n",
    "print('Verify series was appended:', len(recession_months))\n",
    "\n",
    "recessions.iloc[15:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special thanks to Mortada Mehyar for creating [fredapi](https://github.com/mortada/fredapi). This saved some time from having to deal with JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
